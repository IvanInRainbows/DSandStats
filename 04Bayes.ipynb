{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LesZZdzyq5dZ",
        "outputId": "6a759835-96be-4e51-d993-c052a6058f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import math\n",
        "import csv\n",
        "nltk.download(\"punkt\")\n",
        "pd.options.mode.chained_assignment = None  # default='warn'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes\n",
        "\n",
        "El teorema de bayes es:\n",
        "\n",
        "    P(x dado y) = (P(y and x)*P(x))/P(y)\n",
        "\n",
        "Tenemos que asumir que podemos representar x como un conjunto de caracteristicas, por lo que:\n",
        "\n",
        "    P(y|x) = (P(f1,f2,f3... fn | y)*P(x))/P(y)\n",
        "    P(f1,f2,f3... fn | y) = P(f1 | y) * P(f2 | y) * P(f3 | y)...P(fn | y)\n",
        "\n",
        "Estas caracteristicas pueden ser las palabras de la que consta un documento que queremos clasificar, por ejemplo, y asumimos que estas caracteristicas o features son independientes entre si para poder computarlo. Muchas veces se usa el sumatorio de los logaritmos porque si se usa el producto salen numeros muy peque침os.\n",
        "\n",
        "P(x) es el numero de elementos de cada clase partido por el numero de elementos en general o el numero de muestras.\n",
        "\n",
        "P(fx | clase) es el numero de casos que se da esa caractyeristica dentro de la clase partido del sumatorio de todas las caracteristicas de cada clase, en nuestro caso es el numero de veces que ocurre una palabra dentro de una clase partido del numero de palabras de la clase (al menos de las relevantes para las features)\n",
        "\n",
        "Ya en la parte de testeo, para averiguar a que clase pertenece un elemento se hace el producto de todas las caracteristicas del elemento para cada clase y se multiplica por la probabilidad de cada clase y el que sea mayor es el que tiene mayor probabilidad.\n",
        "\n",
        "Nota: En caso de hacerlo en un espacio logaritmico habria que hacer el sumatorio en lugar del producto.\n",
        "\n",
        "##Binary naive bayes\n",
        "Es una alternativa que, al parecer, mejora en sentiment analysis. Se basa en la idea de que para este analisis es mas relevante el hecho de que una palabra ocurra frente a cuantas veces ocurre la palabra, por lo que limita las veces que se cuenta esa palabra en una misma muestra a 1, es decir, si tenemos una muestra que es \"La pelicula tiene una gran direccion y una gran fotografia\" la palabra \"gran\" solo se cuenta una vez en esta muestra.\n",
        "\n",
        "Tambien tiene en cuenta la negaci칩n y lo que se niega, de forma que si, por ejemplo, se niega algo malo pasa a ser algo bueno, es decir, \"not bad\" no se toma como dos tokens (\"not\" y \"bad\") si no como uno solo \"NOT_bad\".\n",
        "\n",
        "Cabe mencionar que para la clasificacion de textos se pueden usar lexicones anotados, que tienen listas de palabras relacionadas con su percepci칩n positiva o negativa.\n",
        "\n",
        "##Evaluaci칩n del modelo\n",
        "\n",
        "Para evaluar el modelo se usan la precision, recall y F-measure.\n",
        "\n",
        "1. La precision se refiere al ratio de positivos verdaderos frente a todos los positivos dados por el modelo, es decir, que fraccion de los positivos son verdaderos.\n",
        "\n",
        "2. El recall se refiere al ratio entre positivos verdaderos entre todos los positivos de los datos anotados"
      ],
      "metadata": {
        "id": "P2s4JavErQ9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classes"
      ],
      "metadata": {
        "id": "sTpX7zT65oJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesModel():\n",
        "  def __init__(self, data):\n",
        "    self.data = self.computeData(data)\n",
        "    self.p_c = self.classProbability(data)\n",
        "\n",
        "  def classProbability(self, data):\n",
        "    \"\"\"\n",
        "    Returns the prior probability, the probability of a random element belonging to each class, as a dictionary\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for i in data:\n",
        "      if i[1] not in out.keys():\n",
        "        out.update({i[1]:1})\n",
        "      else:\n",
        "        out[i[1]]+=1\n",
        "    n = sum(out.values())\n",
        "    for k in out.keys():\n",
        "      out[k] /= n\n",
        "    return out\n",
        "\n",
        "  def computeProb(self, testdata, log = True):\n",
        "    \"\"\"\n",
        "    Default method when the object is called\n",
        "    Returns the probability of the input belonging to each class\n",
        "    \"\"\"\n",
        "    probabilities = {}\n",
        "    if log:\n",
        "      for c in self.data.columns:\n",
        "        probabilities.update({c : 1})\n",
        "        for w in re.sub(\"[^\\w\\s]\", \"\", testdata).lower().split():\n",
        "          if w in self.data.index:\n",
        "            probabilities[c] += np.log(self.data[c][w])\n",
        "        probabilities[c] += np.log(self.p_c[c])\n",
        "    else:\n",
        "      for c in self.data.columns:\n",
        "        probabilities.update({c : 1})\n",
        "        for w in re.sub(\"[^\\w\\s]\", \"\", testdata).lower().split():\n",
        "          if w in self.data.index:\n",
        "            probabilities[c] *= self.data[c][w]\n",
        "        probabilities[c] *= self.p_c[c]\n",
        "    return probabilities\n",
        "\n",
        "  def computeData(self, data):\n",
        "    \"\"\"\n",
        "    Data must be in a list of tuples with shape (n, 2).\n",
        "    The index one of the tuple must be the review and the second the sentiment\n",
        "    Measures the probability of each word given the class and returns a dataframe.\n",
        "    The calculation follows the basic naive bayes algorithm and uses Laplace smoothing\n",
        "    \"\"\"\n",
        "    out = pd.DataFrame()\n",
        "    for i in data:\n",
        "      if i[1] not in out.columns:\n",
        "        out.insert(len(out.columns), i[1], 0)\n",
        "\n",
        "      for w in re.sub(\"[^\\w\\s]\", \"\", i[0]).lower().split():\n",
        "        if w not in out.index:\n",
        "          dic = {w: [0 for i in out.columns]}\n",
        "          out.loc[w] = float(0)\n",
        "        out[i[1]][w] += 1\n",
        "\n",
        "    out += 1\n",
        "\n",
        "    for c in out.columns:\n",
        "      out[c] = out[c]/sum(out[c])\n",
        "\n",
        "    return out #out.apply(np.log) si queremos hacer el logaritmo de un DataFrame\n",
        "\n",
        "  def debug(self): #Simple debug\n",
        "    print(self.data)\n",
        "    print()\n",
        "    print(self.p_c)\n",
        "\n",
        "  __call__=computeProb\n",
        "\n",
        "class BinaryNaiveBayes(NaiveBayesModel):\n",
        "  def __init__(self, data):\n",
        "    super().__init__(data)\n",
        "\n",
        "  def computeData(self, data):\n",
        "    \"\"\"\n",
        "    Data must be in a list of tuples with shape (n, 2).\n",
        "    The index one of the tuple must be the review and the second the sentiment\n",
        "    Measures the probability of each word given the class and returns a dataframe.\n",
        "    The calculation follows the binary naive bayes algorithm and uses Laplace smoothing\n",
        "    \"\"\"\n",
        "    out = pd.DataFrame()\n",
        "    for i in data:\n",
        "      if i[1] not in out.columns:\n",
        "        out.insert(len(out.columns), i[1], 0)\n",
        "\n",
        "      for w in list(dict.fromkeys(re.sub(\"[^\\w\\s]\", \"\", i[0]).lower().split())):\n",
        "        if w not in out.index:\n",
        "          dic = {w: [0 for i in out.columns]}\n",
        "          out.loc[w] = float(0)\n",
        "        out[i[1]][w] += 1\n",
        "\n",
        "    out += 1\n",
        "\n",
        "    for c in out.columns:\n",
        "      out[c] = out[c]/sum(out[c])\n",
        "\n",
        "    return out #out.apply(np.log) si queremos hacer el logaritmo de un DataFrame\n",
        "\n",
        "    def computeProb(self, testdata, log = True):\n",
        "      probabilities = {}\n",
        "      words = re.sub(\"[^\\w\\s]\", \"\", testdata).lower().split()\n",
        "      words = list(dict.fromkeys(words))\n",
        "      if log:\n",
        "        for c in self.data.columns:\n",
        "          probabilities.update({c : 1})\n",
        "          for w in words:\n",
        "            if w in self.data.index:\n",
        "              probabilities[c] += np.log(self.data[c][w])\n",
        "          probabilities[c] += np.log(self.p_c[c])\n",
        "      else:\n",
        "        for c in self.data.columns:\n",
        "          probabilities.update({c : 1})\n",
        "          for w in words:\n",
        "            if w in self.data.index:\n",
        "              probabilities[c] *= self.data[c][w]\n",
        "          probabilities[c] *= self.p_c[c]\n",
        "      return probabilities\n",
        "\n",
        "    out += 1\n",
        "\n",
        "    for c in out.columns:\n",
        "      out[c] = out[c]/sum(out[c])\n",
        "\n",
        "    return out #out.apply(np.log) si queremos hacer el logaritmo de un DataFrame\n",
        "\n",
        "class MovieReviewModel(BinaryNaiveBayes):\n",
        "  \"\"\"\n",
        "  This is the main model for the movie review sentiment prediction.\n",
        "  It inherits from the BinaryReviewModel which inherits from NaiveBayesModel\n",
        "  The model requires a list of tuples as an input which should be of shape(n, 2)\n",
        "  The test data must be on the same format.\n",
        "  When the object is called the function \"computeProb\" is executed, which requires a single review as input and predicts its sentiment.\n",
        "  \"\"\"\n",
        "  def __init__(self, data):\n",
        "    super().__init__(data)\n",
        "  def comparePredict(self, testdata):\n",
        "    \"\"\"\n",
        "    Makes a prediction given a test data\n",
        "    Compares the predicted system output data to the 'gold standard' data.\n",
        "    Returns a dictionary with the recall, precision, and accuracy inside.\n",
        "    \"\"\"\n",
        "    prediction = []\n",
        "    for i in testdata:\n",
        "      temp = self.computeProb(i[0])\n",
        "      prediction.append((i[0], max(zip(temp.values(), temp.keys()))[1]))\n",
        "\n",
        "    tp = 1\n",
        "    tn = 1\n",
        "    fp = 1\n",
        "    fn = 1\n",
        "\n",
        "    for i in range(len(prediction)):\n",
        "      if prediction[i][1] == testdata[i][1]:\n",
        "        if prediction[i][1] == \"positive\":\n",
        "          tp+=1\n",
        "        else:\n",
        "          tn+=1\n",
        "      else:\n",
        "        if prediction[i][1] == \"positive\":\n",
        "          fp+=1\n",
        "        else:\n",
        "          fn+=1\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "    print(f\"\"\"\n",
        "          True positives = {tp-1}\n",
        "          True negatives = {tn-1}\n",
        "          False positives = {fp-1}\n",
        "          False negatives = {fn-1}\n",
        "          Precision = {precision}\n",
        "          Recall = {recall}\n",
        "          Accuracy = {(acc)*100}%\n",
        "    \"\"\")\n",
        "    return {\"precision\" : precision, \"recall\" : recall, \"accuracy\" : acc}"
      ],
      "metadata": {
        "id": "T7tvE2r9Zmop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercises"
      ],
      "metadata": {
        "id": "tqJoTYIP5q7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 4.2\n",
        "data = [(\"fun, couple, love, love\" , \"comedy\"),\n",
        "        (\"fun, furious, shoot\", \"action\"),\n",
        "        (\"couple, fly, fast, fun, fun\", \"comedy\"),\n",
        "        (\"furious, shoot, shoot, fun\", \"action\"),\n",
        "        (\"fly, fast, shoot, love\", \"action\")]\n",
        "\n",
        "inp = \"fast, couple, shoot, fly\"\n",
        "\n",
        "modelNormal = NaiveBayesModel(data)\n",
        "modelNormal.debug()\n",
        "print(modelNormal(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNqMaCrPpncX",
        "outputId": "98064bdd-5f91-474d-9d7b-e497e21a63ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         comedy    action\n",
            "fun      0.2500  0.166667\n",
            "couple   0.1875  0.055556\n",
            "love     0.1875  0.111111\n",
            "furious  0.0625  0.166667\n",
            "shoot    0.0625  0.277778\n",
            "fly      0.1250  0.111111\n",
            "fast     0.1250  0.111111\n",
            "\n",
            "{'comedy': 0.4, 'action': 0.6}\n",
            "{'comedy': -8.521738971045279, 'action': -8.076580381796658}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_binary = BinaryNaiveBayes(data)\n",
        "model_binary.debug()\n",
        "print(model_binary(inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtvvVUcYqux-",
        "outputId": "80060d3b-a548-4944-8312-21136fde8126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           comedy    action\n",
            "fun      0.214286  0.176471\n",
            "couple   0.214286  0.058824\n",
            "love     0.142857  0.117647\n",
            "furious  0.071429  0.176471\n",
            "shoot    0.071429  0.235294\n",
            "fly      0.142857  0.117647\n",
            "fast     0.142857  0.117647\n",
            "\n",
            "{'comedy': 0.4, 'action': 0.6}\n",
            "{'comedy': -7.98761340054719, 'action': -8.071090277751074}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 4.3\n",
        "data = [\n",
        "    (\"Good, good, good, great, great, great\", \"positive\"),\n",
        "    (\"poor, great, great\", \"positive\"),\n",
        "    (\"good, pOor, poor, poor\", \"negative\"),\n",
        "    (\"good, poor, poor, poor, poor, poor, great, great\", \"negative\"),\n",
        "    (\"poor, poor\", \"negative\")\n",
        "]\n",
        "\n",
        "inp = \"a good, good plot and great characters, but poor acting\"\n",
        "\n",
        "print(\"###### Naive bayes model ######\")\n",
        "modelNormal = NaiveBayesModel(data)\n",
        "modelNormal.debug()\n",
        "result = modelNormal(inp)\n",
        "print(result)\n",
        "print(f\"There's a higher chance of in being {max(zip(result.values(), result.keys()))[1]}\")\n",
        "print(\"\\n###### Binary naive bayes model ######\")\n",
        "model_binary = BinaryNaiveBayes(data)\n",
        "model_binary.debug()\n",
        "result = model_binary(inp)\n",
        "print(result)\n",
        "print(f\"There's a higher chance of ii being {max(zip(result.values(), result.keys()))[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OEo-_z3JwAa",
        "outputId": "143644aa-e022-432c-bc28-09281bb5508e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###### Naive bayes model ######\n",
            "       positive  negative\n",
            "good   0.333333  0.176471\n",
            "great  0.500000  0.176471\n",
            "poor   0.166667  0.647059\n",
            "\n",
            "{'positive': 0.4, 'negative': 0.6}\n",
            "{'positive': -4.598421958998375, 'negative': -5.149946861188155}\n",
            "There's a higher chance of in being positive\n",
            "\n",
            "###### Binary naive bayes model ######\n",
            "       positive  negative\n",
            "good   0.285714  0.333333\n",
            "great  0.428571  0.222222\n",
            "poor   0.285714  0.444444\n",
            "\n",
            "{'positive': 0.4, 'negative': 0.6}\n",
            "{'positive': -4.521877497747463, 'negative': -4.0230578140948134}\n",
            "There's a higher chance of in being negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment model\n",
        "\n",
        "Now I should test this model on a real dataset to see how it works with more data."
      ],
      "metadata": {
        "id": "LvT9Pg-HQVTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/IMDB.csv\", index_col=False, header=0, encoding=\"UTF-8\", on_bad_lines=\"skip\", engine=\"python\")\n",
        "\n",
        "for i in range(len(data)):\n",
        "  data[\"review\"][i] = re.sub(\"<br />\", \" \", data[\"review\"][i])\n",
        "  data[\"review\"][i] = re.sub(\"\\\"\", \"'\", data[\"review\"][i])\n",
        "\n",
        "data = list(data.itertuples(index=False, name=None))"
      ],
      "metadata": {
        "id": "kzSEZz-4QeID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_point = int(0.8 * len(data))\n",
        "train_data, test_data = data[:split_point], data[split_point:]\n",
        "model = MovieReviewModel(train_data[:3000])\n",
        "model.debug()\n",
        "model.comparePredict(test_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oyzgZ54d9jh",
        "outputId": "fc06750e-ce0f-4001-b262-770f92da5781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               positive  negative\n",
            "one            0.003280  0.003360\n",
            "of             0.005825  0.005850\n",
            "the            0.006103  0.006093\n",
            "other          0.001654  0.001550\n",
            "reviewers      0.000065  0.000058\n",
            "...                 ...       ...\n",
            "fiftyfive      0.000004  0.000008\n",
            "enquanto       0.000004  0.000008\n",
            "ela            0.000004  0.000008\n",
            "fora           0.000004  0.000008\n",
            "unrecommended  0.000004  0.000008\n",
            "\n",
            "[36424 rows x 2 columns]\n",
            "\n",
            "{'positive': 0.5026666666666667, 'negative': 0.49733333333333335}\n",
            "\n",
            "          True positives = 38\n",
            "          True negatives = 46\n",
            "          False positives = 9\n",
            "          False negatives = 7\n",
            "          Precision = 0.7959183673469388\n",
            "          Recall = 0.8297872340425532\n",
            "          Accuracy = 82.6923076923077%\n",
            "    \n"
          ]
        }
      ]
    }
  ]
}